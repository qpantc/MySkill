{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This script is made to preprocess raw data from Fluxnet sites. It reads in zipped\n",
    "raw-data and outputs .csv-files with forcing data to LPJ-GUESS and benchmark-data\n",
    "for the post-processing script.\n",
    "\n",
    "Written by: Adrian Gustafson\n",
    "Date: 2018-06-12\n",
    "\"\"\"\n",
    "import os\n",
    "from zipfile import ZipFile\n",
    "from datetime import datetime\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_dates(row):\n",
    "    # Convert the Timestamp from double to string for slicing\n",
    "    timestamp = str(row['TIMESTAMP'].tolist())[:-2]\n",
    "    row['Year'] = timestamp[:4]\n",
    "    fmt = '%Y%m%d'\n",
    "    \n",
    "    if len(timestamp) == 6:\n",
    "        # Monthly values\n",
    "        row['Month'] = timestamp[4:]\n",
    "    elif len(timestamp) == 8:\n",
    "        # daily values\n",
    "        dt = datetime.strptime(timestamp, fmt)\n",
    "        # Check if the date is a leap year, in that case retun None so that the\n",
    "        # row can be dropped later.\n",
    "        if timestamp[4:] == '0229':\n",
    "            row['Month'] = None\n",
    "            row['Day'] = None\n",
    "            row['MonthDay'] = None\n",
    "            row['DOY'] = None\n",
    "        else:\n",
    "            row['Month'] = timestamp[4:6]\n",
    "            row['Day']   = timestamp[6:]\n",
    "            row['MonthDay'] = timestamp[4:]\n",
    "            # LPJ-GUESS day start at 0 but julian days start at 1\n",
    "            row['DOY'] = int(dt.strftime('%j')) - 1\n",
    "    elif len(timestamp) == 12:\n",
    "        fmt = '%Y%m%d%H%M'\n",
    "        # hourly values\n",
    "        dt = datetime.strptime(timestamp, fmt)\n",
    "        # Check if the date is a leap year, in that case retun None so that the\n",
    "        # row can be dropped later.\n",
    "        if timestamp[4:] == '0229':\n",
    "            row['Month'] = None\n",
    "            row['Day'] = None\n",
    "            row['MonthDay'] = None\n",
    "            row['DOY'] = None\n",
    "            row['Hour'] = None\n",
    "        else:\n",
    "            row['Month'] = timestamp[4:6]\n",
    "            row['Day']   = timestamp[6:]\n",
    "            row['MonthDay'] = timestamp[4:8]\n",
    "            # LPJ-GUESS day start at 0 but julian days start at 1\n",
    "            row['DOY'] = int(dt.strftime('%j')) - 1\n",
    "            row['Hour'] = int(timestamp[4:8])/30\n",
    "    else:\n",
    "        # Should not happen\n",
    "        print('invalid timeformat at timestamp {}'.format(timestamp))\n",
    "        \n",
    "    return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "SETTINGS\n",
    "\"\"\" \n",
    "BASE_DIR   = '/Users/quan/projects/MySkill/model_data_fusion/Input' #'/dauta/FLUXNET_from_Michael/fluxnet benchmark/'\n",
    "INPUT_DIR  = 'zip'\n",
    "OUTPUT_DIR = 'US-Me2'\n",
    "\n",
    "sitelist_fn = 'fluxnet2015_sitelist.csv'\n",
    "LAI = 6.2\n",
    "\n",
    "# \n",
    "# forcing_cols   = ['TA_ERA', 'SW_IN_ERA', 'P_ERA']\n",
    "# benchmark_cols = ['NEE_VUT_REF', 'GPP_NT_VUT_REF', 'LE_F_MDS']\n",
    "\n",
    "forcing_cols   = ['TA_ERA', 'SW_IN_ERA', 'SWC_F_MDS_1','VPD_ERA','P_ERA', 'NETRAD', 'LE_F_MDS']\n",
    "benchmark_cols = ['NEE_VUT_REF', 'GPP_NT_VUT_REF', 'LE_F_MDS']\n",
    "input_cols = ['TIMESTAMP'] + forcing_cols + benchmark_cols\n",
    "\n",
    "# Rename input variables according to this map.\n",
    "\n",
    "# '',\n",
    "\n",
    "column_map = {\n",
    "    'TA_ERA': 'TEMP',\n",
    "    'SW_IN_ERA': 'Rad',\n",
    "        \n",
    "    'SWC_F_MDS_1': 'SOILM',\n",
    "    'VPD_ERA': 'VPD',\n",
    "    'P_ERA': 'P', \n",
    "    'NETRAD': 'RNET', \n",
    "    'LE_F_MDS': 'ET',\n",
    "    \n",
    "    'NEE_VUT_REF': 'NEE',\n",
    "    'GPP_NT_VUT_REF': 'GPP',\n",
    "    'Lon': 'Lon',\n",
    "    'Lat': 'Lat',\n",
    "    'IGBP': 'IGBP',\n",
    "    'DOY': 'DOY'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data from site US-Me2\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "SCRIPT STARTS\n",
    "\"\"\"\n",
    "\n",
    "sitelist = pd.read_csv(os.path.join(BASE_DIR, sitelist_fn), encoding = \"ISO-8859-1\")\n",
    "\n",
    "daily = pd.DataFrame()\n",
    "monthly = pd.DataFrame()\n",
    "hourly = pd.DataFrame()\n",
    "for fn in os.listdir(os.path.join(BASE_DIR, INPUT_DIR)):\n",
    "    site = fn.split('_')[1]\n",
    "    print('Extracting data from site {}'.format(site))\n",
    "    site_data = sitelist[sitelist['SITE_ID'] == site]\n",
    "    try:\n",
    "        zf = ZipFile(os.path.join(BASE_DIR, INPUT_DIR, fn))\n",
    "    except:\n",
    "        print('Bad file at site {}'.format(site))\n",
    "        continue\n",
    "        \n",
    "    for f in zf.namelist():\n",
    "        if '_FULLSET_DD_' in f :\n",
    "            daily_raw = f\n",
    "            \n",
    "        if '_FULLSET_MM_' in f :\n",
    "            monthly_raw = f\n",
    "        if '_FULLSET_HH_' in f :\n",
    "            hourly_raw = f\n",
    "        \n",
    "    df_daily = pd.read_csv(zf.open(daily_raw), usecols=input_cols)\n",
    "    df_monthly = pd.read_csv(zf.open(monthly_raw), usecols=input_cols)\n",
    "    df_hourly = pd.read_csv(zf.open(hourly_raw), usecols=['TIMESTAMP_START'] + forcing_cols + benchmark_cols)\n",
    "    df_hourly = df_hourly.rename(columns={'TIMESTAMP_START':'TIMESTAMP'})\n",
    "    \n",
    "    df_daily = df_daily.apply(format_dates, axis=1)\n",
    "    df_monthly = df_monthly.apply(format_dates, axis=1)\n",
    "    df_hourly = df_hourly.apply(format_dates, axis=1)\n",
    "\n",
    "    # Drop 29th of February\n",
    "    df_daily.dropna(inplace=True)\n",
    "    df_monthly.dropna(inplace=True)\n",
    "    df_hourly.dropna(inplace=True)\n",
    "\n",
    "    \n",
    "    lon = site_data['LOCATION_LONG'].values[0]\n",
    "    lat = site_data['LOCATION_LAT'].values[0]\n",
    "    igbp = site_data['IGBP'].values[0]\n",
    "    \n",
    "    df_daily['Lon'] = lon; df_daily['Lat'] = lat; df_daily['IGBP'] = igbp\n",
    "    df_monthly['Lon'] = lon; df_monthly['Lat'] = lat; df_monthly['IGBP'] = igbp\n",
    "    df_hourly['Lon'] = lon; df_hourly['Lat'] = lat; df_hourly['IGBP'] = igbp\n",
    "\n",
    "    if len(df_daily) % 365 != 0:\n",
    "        print('Skipping location at site {}. Not full years'.format(site))\n",
    "        continue\n",
    "    \n",
    "    daily = pd.concat([daily, df_daily[['Lon', 'Lat', 'Year', 'DOY', 'IGBP']+benchmark_cols]], ignore_index=True)\n",
    "    monthly = pd.concat([monthly, df_monthly[['Lon', 'Lat', 'Year', 'Month', 'IGBP']+benchmark_cols]], ignore_index=True)\n",
    "    hourly = pd.concat([hourly, df_hourly[['Lon', 'Lat', 'Year', 'Month', 'IGBP']+benchmark_cols]], ignore_index=True)\n",
    "\n",
    "    # df_daily[['Year', 'MonthDay']+forcing_cols].to_csv(os.path.join(BASE_DIR, OUTPUT_DIR, '{}.csv'.format(site)),\n",
    "    #                                                     sep='\\t',\n",
    "    #                                                     index=False, #header=False,\n",
    "    #                                                     float_format='%.3f')\n",
    "    \n",
    "daily.rename(columns=column_map, inplace=True)\n",
    "monthly.rename(columns=column_map, inplace=True)\n",
    "hourly.rename(columns=column_map, inplace=True)\n",
    "\n",
    "daily.to_csv(os.path.join(BASE_DIR, OUTPUT_DIR, 'daily.csv'), sep='\\t', index=False)\n",
    "monthly.to_csv(os.path.join(BASE_DIR, OUTPUT_DIR, 'monthly.csv'), sep='\\t', index=False)\n",
    "hourly.to_csv(os.path.join(BASE_DIR, OUTPUT_DIR, 'monthly.csv'), sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished!\n"
     ]
    }
   ],
   "source": [
    "df_hourly_for_AMIS = df_hourly[['Year', 'MonthDay','DOY']+forcing_cols+benchmark_cols].rename(columns=column_map)\n",
    "\n",
    "df_hourly_for_AMIS['SOILM2'] = df_hourly_for_AMIS['SOILM'] \n",
    "df_hourly_for_AMIS['LAI'] = LAI\n",
    "df_hourly_for_AMIS['Vegk'] = 1 # Vegk the canopy extinction coefficient calculated using solar zenith angle assuming a spherical leaf angle distribution\n",
    "\n",
    "df_hourly_for_AMIS['GA'] = 1\n",
    "df_hourly_for_AMIS['GA_U'] = 1\n",
    "df_hourly_for_AMIS['GSOIL'] = 1\n",
    "\n",
    "df_hourly_for_AMIS.to_csv(os.path.join(BASE_DIR, '{}.csv'.format(site)),\n",
    "                        index=False, #header=False,\n",
    "                        float_format='%.3f')\n",
    "\n",
    "print('Finished!')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
