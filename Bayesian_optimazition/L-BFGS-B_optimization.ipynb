{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# l-bfgs-b algorithm local optimization of a convex function\n",
    "from scipy.optimize import minimize\n",
    "from scipy.optimize import rosen, rosen_der\n",
    "import numpy as np\n",
    "from matplotlib import cm\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "np.random.seed(122)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def plot_objective(objective):\n",
    "    # Initialize figure \n",
    "    figRos = plt.figure(figsize=(12, 7))\n",
    "    axRos = plt.subplot(111, projection='3d')\n",
    "\n",
    "    # Evaluate function\n",
    "    X = np.arange(-1, 1, 0.15)\n",
    "    Y = np.arange(-1, 1, 0.15)\n",
    "    X, Y = np.meshgrid(X, Y)\n",
    "    XX = (X,Y)\n",
    "    Z = objective(XX)\n",
    "\n",
    "    # Plot the surface\n",
    "    surf = axRos.plot_surface(X, Y, Z, cmap=cm.gist_heat_r,\n",
    "                        linewidth=0, antialiased=False)\n",
    "    axRos.set_zlim(0, 50)\n",
    "    figRos.colorbar(surf, shrink=0.5, aspect=10)\n",
    "    plt.savefig('objective_function.png',bbox_inches='tight', dpi=200)\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "## Rosenbrock function\n",
    "# objective function\n",
    "b = 10\n",
    "def objective(x):\n",
    "    f = (x[0]-1)**2 + b*(x[1]-x[0]**2)**2\n",
    "    return f\n",
    "\n",
    "\n",
    "plot_objective(objective)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial input pt:  [-0.68601632  0.40442008]\n",
      "Total time taken for the minimization: 0.0043s\n",
      "Status : CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL\n",
      "Total Evaluations: 24\n",
      "Solution: f([1.0000006  1.00000115]) = 0.00000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# derivative of the objective function\n",
    "def derivative(x):\n",
    "    df = np.array([2*(x[0]-1) - 4*b*(x[1] - x[0]**2)*x[0], \\\n",
    "                         2*b*(x[1]-x[0]**2)])\n",
    "    return df\n",
    "\n",
    "\n",
    "starttime = time.perf_counter()\n",
    "# define range for input\n",
    "r_min, r_max = -1.0, 1.0\n",
    "\n",
    "# define the starting point as a random sample from the domain\n",
    "pt = r_min + np.random.rand(2) * (r_max - r_min)\n",
    "print('initial input pt: ', pt)\n",
    "\n",
    "# perform the l-bfgs-b algorithm search\n",
    "result = minimize(objective, pt, method='L-BFGS-B', jac=derivative)\n",
    "print(f\"Total time taken for the minimization: {time.perf_counter()-starttime:.4f}s\")\n",
    "# summarize the result\n",
    "print('Status : %s' % result['message'])\n",
    "print('Total Evaluations: %d' % result['nfev'])\n",
    "\n",
    "\n",
    "# evaluate solution\n",
    "solution = result['x']\n",
    "evaluation = objective(solution)\n",
    "print('Solution: f(%s) = %.5f' % (solution, evaluation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time taken for the minimization: 0.0091s\n",
      "The local minimum occurs at point [0.99991699 0.99983066] for iteration: 2094\n"
     ]
    }
   ],
   "source": [
    "# Gradient descent method\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "starttime = time.perf_counter()\n",
    "\n",
    "# define range for input\n",
    "r_min, r_max = -1.0, 1.0\n",
    "\n",
    "# define the starting point as a random sample from the domain\n",
    "cur_x = r_min + np.random.rand(2) * (r_max - r_min)\n",
    "\n",
    "rate = 0.01 # Learning rate\n",
    "precision = 0.000001 #This tells us when to stop the algorithm\n",
    "previous_step_size = 1 #\n",
    "max_iters = 10000 # maximum number of iterations\n",
    "iters = 0 #iteration counter\n",
    "\n",
    "\n",
    "## Rosenbrock function\n",
    "# objective function\n",
    "b = 10\n",
    "def objective(x):\n",
    "    f = (x[0]-1)**2 + b*(x[1]-x[0]**2)**2\n",
    "    return f\n",
    "\n",
    "\n",
    "# derivative of the objective function\n",
    "def derivative(x):\n",
    "    df = np.array([2*(x[0]-1) - 4*b*(x[1] - x[0]**2)*x[0], \\\n",
    "                         2*b*(x[1]-x[0]**2)])\n",
    "    return df\n",
    "\n",
    "while previous_step_size > precision and iters < max_iters:\n",
    "    prev_x = cur_x #Store current x value in prev_x\n",
    "    cur_x = cur_x - rate * derivative(prev_x) #Grad descent\n",
    "    previous_step_size = sum(abs(cur_x - prev_x)) #Change in x\n",
    "    iters = iters+1 #iteration count\n",
    "print(f\"Total time taken for the minimization: {time.perf_counter()-starttime:.4f}s\")\n",
    "print(\"The local minimum occurs at point\", cur_x, \"for iteration:\", iters)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
